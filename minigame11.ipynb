{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minigame 11: Choose One Element To Refine And Advect Mesh\n",
    "\n",
    "This is like minigame10, except that we're now \"advecting\" mesh by a rigid translation. This is like a \"remap\" method, where the \"remap\" is just to re-project the known function onto the new mesh.\n",
    "\n",
    "Some things to explore:\n",
    "\n",
    "* PPO vs DQN vs ?\n",
    "* CNN vs MLP vs ?\n",
    "* order=1 vs order=2 vs ?\n",
    "* H1 space vs DG space vs ?\n",
    "\n",
    "Setup PyMFEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos,sin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "from gym import spaces, utils\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from os.path import expanduser, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyglvis import GlvisWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mfem import path\n",
    "import mfem.ser as mfem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 21:09:45,789\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_workers': 3,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 10000,\n",
       " 'model': {'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {},\n",
       " 'env': None,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tfe',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " '_use_trajectory_view_api': True,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent'},\n",
       " 'logger_config': None,\n",
       " 'replay_sequence_length': 1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': False,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "# This env setting is necessary to avoid problems within rllib due to serialization and workers\n",
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['train_batch_size'] = int(1e4)\n",
    "config['num_workers'] = 3\n",
    "config['framework'] = 'tfe'\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solnstream(mesh,soln):\n",
    "    mesh.Print(\",tmpmesh\")\n",
    "    with open(\",tmpmesh\",\"r\") as f:\n",
    "        meshdata = f.read()\n",
    "    soln.Save(\",tmpsoln\")\n",
    "    with open(\",tmpsoln\",\"r\") as f:\n",
    "        solndata = f.read()\n",
    "    solndata = \"solution\\n\"+meshdata+solndata\n",
    "    return solndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some synthetic test functions: steps and bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x,theta):\n",
    "    x0 = x[0]\n",
    "    y0 = x[1]\n",
    "    x1 = x0*cos(theta)-y0*sin(theta)\n",
    "    y1 = x0*sin(theta)+y0*cos(theta)\n",
    "    return [x1,y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    x0 = x[0]\n",
    "    if (x0 < 0.0):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotated_step(x, theta):\n",
    "    xr = rotate(x,theta)\n",
    "    return step(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump(x):\n",
    "    rsq = x[0]**2 +x[1]**2\n",
    "    return math.exp(-rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_step(x):\n",
    "    return 0.5*(1.0 +math.tanh(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotated_smooth_step(x,theta):\n",
    "    xr = rotate(x,theta)\n",
    "    return smooth_step(xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classes where we can set the parameters and then eval a bunch of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.theta = random.uniform(0.0, 2.0*math.pi)\n",
    "        self.dx = [random.uniform(-1.0, 1.0),random.uniform(-1.0, 1.0)]\n",
    "        \n",
    "    def EvalValue(self, x):\n",
    "        return rotated_step(x+self.dx, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bump(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width = random.uniform(0.1,1.0)\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.dx = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return bump((x-self.xc+self.dx)/self.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBump(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width1 = random.uniform(0.1,0.5)\n",
    "        self.width2 = random.uniform(0.1,0.5)\n",
    "        self.xc1 = [0.5,0.5]\n",
    "        self.xc2 = [0.5,0.5]\n",
    "        self.dx1 = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "        self.dx2 = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        #return max(bump((x-self.xc1+self.dx1)/self.width1),bump((x-self.xc2+self.dx2)/self.width2))\n",
    "        return 0.5*(bump((x-self.xc1+self.dx1)/self.width1)+bump((x-self.xc2+self.dx2)/self.width2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothStep(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width = random.uniform(5.0, 10.0)\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.theta = random.uniform(0.0, 2.0*math.pi)\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        x -= self.xc\n",
    "        return rotated_smooth_step(x*self.width, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BumpsAndSmoothStep(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.bump = Bump()\n",
    "        self.bump.SetParams()\n",
    "        self.smooth_step = SmoothStep()\n",
    "        self.smooth_step.SetParams()\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return 0.5*self.bump.EvalValue(x)+0.5*self.smooth_step.EvalValue(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an instance of the test function. Note that each instance has randomly chosen parameters.  For the steps, it's a rotation angle and a displacement.  For the bumps, it's a width and a displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = mfem.Mesh('inline-quad.mesh')\n",
    "mesh.UniformRefinement()\n",
    "mesh.UniformRefinement()\n",
    "mesh.UniformRefinement()\n",
    "fec = mfem.L2_FECollection(p=1, dim=2)\n",
    "fes = mfem.FiniteElementSpace(mesh, fec)\n",
    "u = mfem.GridFunction(fes)\n",
    "c = BumpsAndSmoothStep()\n",
    "c.SetParams()\n",
    "u.ProjectCoefficient(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577e4c17d8b94e3a8f32af8da149430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gl = GlvisWidget(get_solnstream(mesh,u))\n",
    "gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGame(gym.Env):\n",
    "    \n",
    "    class u0_coeff(mfem.PyCoefficient):\n",
    "        \n",
    "        def SetParams(self):\n",
    "            self.fn = BumpsAndSmoothStep()\n",
    "            self.fn.SetParams()\n",
    "            \n",
    "        def EvalValue(self, x):\n",
    "            return self.fn.EvalValue(x)\n",
    "        \n",
    "    # In RLlib, you need the config arg\n",
    "    def __init__(self,config):\n",
    "        self.meshfile = 'inline-quad-7.mesh'\n",
    "        \n",
    "        # keep a copy of the unrefined mesh so we can restore it\n",
    "        self.mesh0 = mfem.Mesh(self.meshfile)\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        \n",
    "        self.u = 0.5\n",
    "        self.v = 0.5\n",
    "        mag = math.sqrt(self.u**2 +self.v**2)\n",
    "        nx = math.sqrt(self.mesh.GetNE())\n",
    "        self.u /= mag\n",
    "        self.u /= nx\n",
    "        self.v /= mag\n",
    "        self.v /= nx\n",
    "        self.displ = mfem.Vector(self.mesh.GetNV()*2)\n",
    "        self.displ.Assign(self.u)\n",
    "        \n",
    "        # The only reason we need to create a fespace and gf here\n",
    "        # is to find the sizes needed for the action and observation spaces\n",
    "        dim = self.mesh.Dimension()\n",
    "        self.order = 1\n",
    "        self.fec = mfem.L2_FECollection(self.order, dim)\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        self.u = mfem.GridFunction(self.fes);\n",
    "\n",
    "        # actions are: refine each element, or do nothing\n",
    "        self.action_space = spaces.Discrete(self.mesh.GetNE())\n",
    "        self.observation_space = spaces.Box(-1.0, 1.0, shape=(self.u.Size(),), dtype=np.float32)\n",
    "        self.state = None\n",
    "        \n",
    "        # call reset to create the first synthetic function\n",
    "        self.reset()\n",
    "        \n",
    "        #self.gl = GlvisWidget(get_solnstream(self.mesh,self.u))\n",
    "        \n",
    "    def get_ne(self):\n",
    "        return self.mesh.GetNE()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.u.Size()\n",
    "    \n",
    "    # Compute L2 error wrt to the analytic fn definition\n",
    "    def get_error(self):\n",
    "        err = self.u.ComputeL2Error(self.u0)\n",
    "        return err\n",
    "    \n",
    "    # Manually refine the elements in the array elems\n",
    "    def refine_elems(self, elems):\n",
    "        self.mesh.GeneralRefinement(mfem.intArray(elems))\n",
    "        self.fes.Update()\n",
    "        self.u.Update()\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "        \n",
    "    def move_mesh(self):\n",
    "        self.mesh.MoveVertices(self.displ)\n",
    "            \n",
    "    # action is the number of the element to refine\n",
    "    def step(self, action):\n",
    "        err1 = self.get_error()\n",
    "        self.move_mesh()\n",
    "        self.refine_elems([action])\n",
    "        err2 = self.get_error()\n",
    "        reward = err1-err2\n",
    "        done = True\n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    # similar to reset, but do not choose a new function\n",
    "    def reinit(self):\n",
    "        del self.mesh\n",
    "        self.mesh = mfem.Mesh(self.mesh0)\n",
    "\n",
    "        del self.fes\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "\n",
    "        del self.u\n",
    "        self.u = mfem.GridFunction(self.fes)\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "        \n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    # every reset of the env chooses a new synthetic function\n",
    "    def reset(self):\n",
    "        self.u0 = self.u0_coeff()\n",
    "        self.u0.SetParams()\n",
    "        return self.reinit()\n",
    "    \n",
    "    def render(self):\n",
    "        return GlvisWidget(get_solnstream(self.mesh,self.u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the environment and sanity check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMRGame(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_ne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397dcfe0d1924f0bbcb70cc1301566fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a061abf56d2f4f22bfae126a5e98eade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with refinement of element 0. Then we'll test resetting it to the original state.  We're going to need this to go through a searching for the best actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe5cac833c64f699a45c4b6113bfbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cc46b1830d4a22afb3681b6945f045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.move_mesh()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9354dc54ce694198bdaefd0ca099eca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reinit() # puts the mesh/fields back in the orig state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, try training a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 21:09:48,715\tINFO trainer.py:588 -- Executing eagerly, with eager_tracing=False\n",
      "2021-02-12 21:09:48,715\tINFO trainer.py:618 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-02-12 21:09:51,095\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"] = \"1\"\n",
    "config['train_batch_size'] = int(1e3)\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m 2021-02-12 21:09:51,120\tWARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m   arr = np.array(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "episode reward mean: 0.000075 \n",
      "CPU times: user 7.26 s, sys: 222 ms, total: 7.48 s\n",
      "Wall time: 16.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24850)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(1):\n",
    "    result = agent.train()\n",
    "    print(\"episode reward mean: %f \" % result[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 196)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          50432       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          50432       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 49)           12593       fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 245,298\n",
      "Trainable params: 245,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convenience function for applying a policy to a given observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24851)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24853)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    }
   ],
   "source": [
    "def apply_policy(model, obs):\n",
    "    action = agent.compute_action(obs, explore=False) # use deterministic mode\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #print(\"policy chooses action %d with reward %f\" % (action, reward))\n",
    "    return action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 0.0004027331533718731)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "action, reward = apply_policy(model, obs)\n",
    "action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 0.0004027331533718731)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reinit()\n",
    "action, reward = apply_policy(model, obs)\n",
    "action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force search for the best choice by trying each one, remembering to reset the environment after each action and after we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_optimal(obs):\n",
    "    u0 = mfem.Vector(obs)\n",
    "    maxr = 0.0;\n",
    "    maxel = -1;\n",
    "    env.reinit()\n",
    "    ne = env.get_ne()\n",
    "    for n in range(ne):\n",
    "        env.reinit()\n",
    "        state, reward, done, info = env.step(n)\n",
    "        if reward > maxr:\n",
    "            maxr = reward\n",
    "            maxel = n\n",
    "    #print(\"max reward is %f by refining element %d\" % (maxr, maxel))\n",
    "    env.reinit()\n",
    "    return maxel, maxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff2e4fbb659441f8df0c2385b74285a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "maxel, maxr = find_optimal(obs)\n",
    "env.refine_elems([maxel])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with what the policy does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a041ff6c690467e813da830f4b5685c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reinit()\n",
    "apply_policy(model,obs)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an error estimator based on the difference between the discontinuous and continuous representations. This is only valid for L2 FE spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dgjumps(env):\n",
    "    \n",
    "    mesh = env.mesh\n",
    "    u = env.u\n",
    "    \n",
    "    # put the L2 gridfunction into a coefficient so we can project it\n",
    "    u_disc_coeff = mfem.GridFunctionCoefficient(u)\n",
    "    h1_fec = mfem.H1_FECollection(p=1, dim=2)\n",
    "    h1_fes = mfem.FiniteElementSpace(mesh, h1_fec)\n",
    "    u_h1 = mfem.GridFunction(h1_fes)\n",
    "    u_h1.ProjectDiscCoefficient(u_disc_coeff, mfem.GridFunction.ARITHMETIC)\n",
    "    \n",
    "    # put the H1 smoothed function into a coefficient\n",
    "    u_h1_coeff = mfem.GridFunctionCoefficient(u_h1)\n",
    "    \n",
    "    # create a 0-order L2 field to hold errors\n",
    "    l2_0_fec = mfem.L2_FECollection(p=0,dim=2)\n",
    "    l2_0_fes = mfem.FiniteElementSpace(mesh,l2_0_fec)\n",
    "\n",
    "    # Compute elementwise \"errors\" between continuous and discontinuous fields\n",
    "    err_gf = mfem.GridFunction(l2_0_fes);\n",
    "    u.ComputeElementL2Errors(u_h1_coeff, err_gf);\n",
    "    \n",
    "    best_action = np.argmax(err_gf.GetDataArray())\n",
    "    \n",
    "    state, reward, done, info = env.step(best_action)\n",
    "    env.reinit()\n",
    "\n",
    "    return best_action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd6d307a1d44d29a37f3879e7fc8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "action, reward = find_dgjumps(env)\n",
    "env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random policy gives us a scale for the low end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_policy(obs):\n",
    "    ne = env.get_ne()\n",
    "    ir = np.random.randint(0,ne)\n",
    "    state, reward, done, info = env.step(ir)\n",
    "    env.reinit()\n",
    "    return ir, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a more systematic evaluation using an ensemble of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(model, ntrials):\n",
    "    \n",
    "    ncorrect = 0\n",
    "    sumsq = 0.0\n",
    "    maxerrsq = 0.0\n",
    "    \n",
    "    dg_ncorrect = 0\n",
    "    dg_sumsq = 0.0\n",
    "    dg_maxerrsq = 0.0\n",
    "    \n",
    "    rand_ncorrect = 0\n",
    "    rand_sumsq = 0.0\n",
    "    rand_maxerrsq = 0.0\n",
    "    for n in range(ntrials):\n",
    "        obs = env.reset()\n",
    "        \n",
    "        bestaction, bestreward = find_optimal(obs)\n",
    "        dgaction, dgreward = find_dgjumps(env)\n",
    "        action, reward = apply_policy(model,obs)\n",
    "        rand_action, rand_reward = apply_random_policy(obs)\n",
    "        \n",
    "        err = bestreward-reward\n",
    "        maxerrsq = max(err**2,maxerrsq)\n",
    "        sumsq += err**2\n",
    "        \n",
    "        dg_err = bestreward-dgreward\n",
    "        dg_maxerrsq = max(dg_err**2,dg_maxerrsq)\n",
    "        dg_sumsq += dg_err**2\n",
    "        \n",
    "        rand_err = bestreward-rand_reward\n",
    "        rand_maxerrsq = max(rand_err**2,rand_maxerrsq)\n",
    "        rand_sumsq += rand_err**2\n",
    "        \n",
    "        if (bestaction == action):\n",
    "            ncorrect += 1\n",
    "        if (bestaction == dgaction):\n",
    "            dg_ncorrect += 1\n",
    "        if (bestaction == rand_action):\n",
    "            rand_ncorrect += 1\n",
    "    \n",
    "    rms = math.sqrt(sumsq/ntrials)\n",
    "    corr = 100.*ncorrect/ntrials\n",
    "    print(\"policy rms error: \",rms,flush=True)\n",
    "    print(\"policy max sq error: \",math.sqrt(maxerrsq),flush=True)\n",
    "    print(\"policy % correct: \",corr,flush=True)\n",
    "    \n",
    "    dg_rms = math.sqrt(dg_sumsq/ntrials)\n",
    "    dg_corr = 100.*dg_ncorrect/ntrials\n",
    "    print(\"dg rms error: \",dg_rms,flush=True)\n",
    "    print(\"dg max sq error: \",math.sqrt(dg_maxerrsq),flush=True)\n",
    "    print(\"dg % correct: \",dg_corr,flush=True)\n",
    "    \n",
    "    rand_rms = math.sqrt(rand_sumsq/ntrials)\n",
    "    rand_corr = 100.*rand_ncorrect/ntrials\n",
    "    print(\"rand rms error: \",rand_rms,flush=True)\n",
    "    print(\"rand max sq error: \",math.sqrt(rand_maxerrsq),flush=True)\n",
    "    print(\"rand % correct: \",rand_corr,flush=True)\n",
    "    \n",
    "    return rms, math.sqrt(maxerrsq), corr, dg_rms, math.sqrt(dg_maxerrsq), dg_corr, rand_rms, math.sqrt(rand_maxerrsq), rand_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "eval_ensemble(model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a few eval sample sizes to get a sense of how many are needed to estimate the metrics of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_ensemble(model, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_ensemble(model, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the training process is making progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-02-12 21:10:10,615\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 0 of size 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25224)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=25224)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=25224)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m 2021-02-12 21:10:12,966\tWARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=24852)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=25224)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=25224)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   arr = np.array(v)\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1.e6\n",
    "nbatches = 20\n",
    "batch_size = total_episodes/nbatches\n",
    "neval = 400\n",
    "\n",
    "del agent\n",
    "config['train_batch_size'] = int(batch_size)\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "rms = [0.0] * nbatches\n",
    "cor = [0.0] * nbatches\n",
    "maxerr = [0.0] * nbatches\n",
    "\n",
    "dg_rms = [0.0] * nbatches\n",
    "dg_cor = [0.0] * nbatches\n",
    "dg_maxerr = [0.0] * nbatches\n",
    "\n",
    "rand_rms = [0.0] * nbatches\n",
    "rand_cor = [0.0] * nbatches\n",
    "rand_maxerr = [0.0] * nbatches\n",
    "\n",
    "for n in range(nbatches):\n",
    "    print(\"training batch %d of size %d\" % (n,batch_size))\n",
    "    agent.train()\n",
    "    print(\"evaluating on %d instances...\" %  neval)\n",
    "    rms[n], maxerr[n], cor[n], dg_rms[n], dg_maxerr[n], dg_cor[n], rand_rms[n], rand_maxerr[n], rand_cor[n] = eval_ensemble(model, neval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "isteps = list(range(nbatches))\n",
    "asteps = [i*config['train_batch_size'] for i in isteps]\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.subplot(211)\n",
    "ax.set_ylim(0.0001,0.01)\n",
    "ax.set_ylabel('Error')\n",
    "line1, = plt.semilogy(asteps,rms[:nbatches], marker='o')\n",
    "line2, = plt.semilogy(asteps,dg_rms[:nbatches], marker='x')\n",
    "line3, = plt.semilogy(asteps,maxerr[:nbatches], marker='.')\n",
    "line4, = plt.semilogy(asteps,dg_maxerr[:nbatches], marker='+')\n",
    "\n",
    "line1.set_label('RL rms')\n",
    "line2.set_label('DG rms')\n",
    "line3.set_label('RL max')\n",
    "line4.set_label('DG max')\n",
    "ax.legend()\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('% correct')\n",
    "ax.set_xlabel('training episodes')\n",
    "line1, = plt.plot(asteps,cor[:nbatches], marker='o')\n",
    "line2, = plt.plot(asteps,dg_cor[:nbatches], marker='x')\n",
    "line1.set_label('RL policy')\n",
    "line2.set_label('DG')\n",
    "ax.legend()\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for cases where the policy gets it right and the DG method gets it wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(500):\n",
    "    obs = env.reset()\n",
    "    opt_action, opt_reward = find_optimal(obs)\n",
    "    dg_action, dg_reward = find_dgjumps(env)\n",
    "    pol_action, pol_reward = apply_policy(model, obs)\n",
    "    if ((pol_action == opt_action) and (dg_action != opt_action)):\n",
    "        break\n",
    "env.reinit()\n",
    "env.step(pol_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reinit()\n",
    "env.step(dg_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigame",
   "language": "python",
   "name": "minigame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
