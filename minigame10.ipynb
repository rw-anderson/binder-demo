{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minigame 10: Choose One Element To Refine\n",
    "\n",
    "This is basically the global environment for refinement where the action is choosing one element at a time.  However, the observation space is the DOFs directly, not function values.\n",
    "\n",
    "Some things to explore:\n",
    "\n",
    "* PPO vs DQN vs ?\n",
    "* CNN vs MLP vs ?\n",
    "* order=1 vs order=2 vs ?\n",
    "* H1 space vs DG space vs ?\n",
    "\n",
    "Setup PyMFEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "from gym import spaces, utils\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from os.path import expanduser, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyglvis import GlvisWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mfem import path\n",
    "import mfem.ser as mfem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "# This env setting is necessary to avoid problems within rllib due to serialization and workers\n",
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['train_batch_size'] = int(5e4)\n",
    "config['framework'] = 'tfe'\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the gym environment. This is essentially the \"global refinement environment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solnstream(mesh,soln):\n",
    "    mesh.Print(\",tmpmesh\")\n",
    "    with open(\",tmpmesh\",\"r\") as f:\n",
    "        meshdata = f.read()\n",
    "    soln.Save(\",tmpsoln\")\n",
    "    with open(\",tmpsoln\",\"r\") as f:\n",
    "        solndata = f.read()\n",
    "    solndata = \"solution\\n\"+meshdata+solndata\n",
    "    return solndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGame(gym.Env):\n",
    "    \n",
    "    class u0_coeff(mfem.PyCoefficient):\n",
    "        \n",
    "        def setw(self,w):\n",
    "            self.w = w\n",
    "            \n",
    "        def EvalValue(self, x):\n",
    "            return math.sin(self.w*math.pi*x[0]) * math.sin(self.w*math.pi*x[1])\n",
    "        \n",
    "    # In RLlib, you need the config arg\n",
    "    def __init__(self,config):\n",
    "        import mfem.ser as mfem\n",
    "\n",
    "        self.meshfile = 'star.mesh'\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        #self.mesh.UniformRefinement()\n",
    "        \n",
    "        dim = self.mesh.Dimension()\n",
    "        order = 1\n",
    "        self.fec = mfem.H1_FECollection(order, dim)\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        self.u = mfem.GridFunction(self.fes);\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.mesh.GetNE())\n",
    "        self.observation_space = spaces.Box(-1.0, 1.0, shape=(self.u.Size(),), dtype=np.float32)\n",
    "        self.state = None\n",
    "        \n",
    "        self.gl = GlvisWidget(get_solnstream(self.mesh,self.u))\n",
    "        \n",
    "    def get_ne(self):\n",
    "        return self.mesh.GetNE()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.u.Size()\n",
    "    \n",
    "    # Compute L2 error wrt to the analytic fn definition\n",
    "    def get_error(self):\n",
    "        err = self.u.ComputeL2Error(self.u0)\n",
    "        return err\n",
    "    \n",
    "    # Manually refine the elements in the array elems\n",
    "    def refine_elems(self, elems):\n",
    "        self.mesh.GeneralRefinement(mfem.intArray(elems))\n",
    "        self.fes.Update()\n",
    "        self.u.Update()\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "    \n",
    "    # Put mesh in original state and reset gridfunction with supplied vector\n",
    "    def reset_to(self,u0):\n",
    "        del self.mesh\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        #self.mesh.UniformRefinement()\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        self.u = mfem.GridFunction(self.fes)\n",
    "        self.u.Assign(u0)\n",
    "\n",
    "    # action is the number of the element to refine\n",
    "    def step(self, action):\n",
    "        err1 = self.get_error()\n",
    "        self.refine_elems([action])\n",
    "        err2 = self.get_error()\n",
    "        reward = err1-err2\n",
    "        done = True\n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.u0 = self.u0_coeff()\n",
    "        self.u0.setw(1.0+5.0*np.random.rand())\n",
    "\n",
    "        # reread the mesh from file - probably want a faster way to do this\n",
    "        del self.mesh\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        #self.mesh.UniformRefinement()\n",
    "        dim = self.mesh.Dimension()\n",
    "        order = 1\n",
    "        del self.fec\n",
    "        self.fec = mfem.H1_FECollection(order, dim)\n",
    "        del self.fes\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        del self.u\n",
    "        self.u = mfem.GridFunction(self.fes)\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "        err = self.get_error()\n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self):\n",
    "        return GlvisWidget(get_solnstream(self.mesh,self.u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the environment and sanity check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMRGame(None)\n",
    "env.get_ne()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_ne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = copy.copy(obs)\n",
    "u0 = mfem.Vector(obs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(0)\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with refinement of element 0. Then we'll test resetting it to the original state.  We're going to need this to go through a searching for the best actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_to(u0) # puts the mesh back in the orig state, and sets the DOF vector to u0\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, try training a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo.PPOTrainer(config, env=AMRGame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "for n in range(2):\n",
    "    result = agent.train()\n",
    "    print(\"episode reward mean: %f \" % result[\"episode_reward_mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_policy(model, obs):\n",
    "    action = agent.compute_action(obs)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #print(\"policy chooses action %d with reward %f\" % (action, reward))\n",
    "    return action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs0 = copy.copy(obs)\n",
    "u0 = mfem.Vector(obs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force search for the best choice by trying each one, remembering to reset the environment after each action and after we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_best_el(obs):\n",
    "    u0 = mfem.Vector(obs)\n",
    "    maxr = 0.0;\n",
    "    maxel = -1;\n",
    "    env.reset_to(u0)\n",
    "    ne = env.get_ne()\n",
    "    for n in range(ne):\n",
    "        env.reset_to(u0)\n",
    "        state, reward, done, info = env.step(n)\n",
    "        if reward > maxr:\n",
    "            maxr = reward\n",
    "            maxel = n\n",
    "    #print(\"max reward is %f by refining element %d\" % (maxr, maxel))\n",
    "    env.reset_to(u0)\n",
    "    return maxel, maxr\n",
    "\n",
    "maxel, maxr = find_best_el(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.refine_elems([maxel])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with what the policy does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset_to(u0)\n",
    "apply_policy(model,obs0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a more systematic evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(model, ntrials):\n",
    "    ncorrect = 0\n",
    "    sumsq = 0\n",
    "    for n in range(ntrials):\n",
    "        obs = env.reset()\n",
    "        bestaction, bestreward = find_best_el(obs)\n",
    "        action, reward = apply_policy(model,obs)\n",
    "        err = bestreward-reward\n",
    "        sumsq += err*err\n",
    "        if (bestaction == action):\n",
    "            ncorrect += 1\n",
    "    rms = math.sqrt(sumsq/ntrials)\n",
    "    corr = 100.*ncorrect/ntrials\n",
    "    print(\"rms error: \",rms,flush=True)\n",
    "    print(\"% correct: \",corr,flush=True)\n",
    "    return rms, corr\n",
    "\n",
    "eval_ensemble(model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the training process is converging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsteps = 20\n",
    "neval = 200\n",
    "\n",
    "del agent\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)\n",
    "\n",
    "rms = [0.0] * nsteps\n",
    "cor = [0.0] * nsteps\n",
    "for n in range(nsteps):\n",
    "    print(\"training batch %d\" % n)\n",
    "    agent.train()\n",
    "    print(\"evaluating on %d instances...\" %  neval)\n",
    "    rms[n], cor[n] = eval_ensemble(model, neval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "isteps = list(range(nsteps))\n",
    "asteps = [i*config['train_batch_size'] for i in isteps]\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.subplot(211)\n",
    "ax.set_ylim(0.001,0.1)\n",
    "ax.set_ylabel('RMS error')\n",
    "plt.semilogy(asteps,rms[:10], marker='o')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax = plt.subplot(212)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('% correct')\n",
    "plt.plot(asteps,cor[:10], marker='o')\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.dqn as dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agent\n",
    "agent = dqn.DQNTrainer(config, env=AMRGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.train()\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 10\n",
    "neval = 200\n",
    "\n",
    "del agent\n",
    "agent = dqn.DQNTrainer(config, env=AMRGame)\n",
    "\n",
    "rms = [0.0] * nsteps\n",
    "cor = [0.0] * nsteps\n",
    "for n in range(10):\n",
    "    agent.train()\n",
    "    print(\"evaluating on %d instances...\" %  neval)\n",
    "    rms[n], cor[n] = eval_ensemble(model, neval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.subplot(211)\n",
    "ax.set_ylim(0.0001,0.1)\n",
    "ax.set_ylabel('RMS error')\n",
    "plt.semilogy(nsteps,rms, marker='o')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax = plt.subplot(212)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('% correct')\n",
    "plt.plot(nsteps,cor, marker='o')\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigame",
   "language": "python",
   "name": "minigame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
