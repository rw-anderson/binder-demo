{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minigame 10: Choose One Element To Refine\n",
    "\n",
    "This is essentially the global environment for refinement where the action is choosing one element at a time.  However, the observation space is the DOFs directly, not function values.\n",
    "\n",
    "Some things to explore:\n",
    "\n",
    "* PPO vs DQN vs ?\n",
    "* CNN vs MLP vs ?\n",
    "* order=1 vs order=2 vs ?\n",
    "* H1 space vs DG space vs ?\n",
    "\n",
    "Setup PyMFEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos,sin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "from gym import spaces, utils\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from os.path import expanduser, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyglvis import GlvisWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mfem import path\n",
    "import mfem.ser as mfem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 20:48:33,865\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 10000,\n",
       " 'model': {'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {},\n",
       " 'env': None,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tfe',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " '_use_trajectory_view_api': True,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent'},\n",
       " 'logger_config': None,\n",
       " 'replay_sequence_length': 1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': False,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "# This env setting is necessary to avoid problems within rllib due to serialization and workers\n",
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['train_batch_size'] = int(1e4)\n",
    "config['framework'] = 'tfe'\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solnstream(mesh,soln):\n",
    "    mesh.Print(\",tmpmesh\")\n",
    "    with open(\",tmpmesh\",\"r\") as f:\n",
    "        meshdata = f.read()\n",
    "    soln.Save(\",tmpsoln\")\n",
    "    with open(\",tmpsoln\",\"r\") as f:\n",
    "        solndata = f.read()\n",
    "    solndata = \"solution\\n\"+meshdata+solndata\n",
    "    return solndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some synthetic test functions: steps and bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x,theta):\n",
    "    x0 = x[0]\n",
    "    y0 = x[1]\n",
    "    x1 = x0*cos(theta)-y0*sin(theta)\n",
    "    y1 = x0*sin(theta)+y0*cos(theta)\n",
    "    return [x1,y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    x0 = x[0]\n",
    "    if (x0 < 0.0):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotated_step(x, theta):\n",
    "    xr = rotate(x,theta)\n",
    "    return step(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump(x):\n",
    "    rsq = x[0]**2 +x[1]**2\n",
    "    return math.exp(-rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classes where we can set the values and then eval a bunch of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.theta = random.uniform(0.0, 2.0*math.pi)\n",
    "        self.dx = [random.uniform(-1.0, 1.0),random.uniform(-1.0, 1.0)]\n",
    "        \n",
    "    def EvalValue(self, x):\n",
    "        return rotated_step(x+self.dx, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bump(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width = random.uniform(0.1,1.0)\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.dx = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return bump((x-self.xc+self.dx)/self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an instance of the test function. Note that each instance has randomly chosen parameters.  For the steps, it's a rotation angle and a displacement.  For the bumps, it's a width and a displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = mfem.Mesh('inline-quad.mesh')\n",
    "fec = mfem.L2_FECollection(p=1, dim=2)\n",
    "fes = mfem.FiniteElementSpace(mesh, fec)\n",
    "u = mfem.GridFunction(fes)\n",
    "c = Bump()\n",
    "c.SetParams()\n",
    "u.ProjectCoefficient(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54bbc0f7add451cabe8fdc016b0848b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gl = GlvisWidget(get_solnstream(mesh,u))\n",
    "gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGame(gym.Env):\n",
    "    \n",
    "    class u0_coeff(mfem.PyCoefficient):\n",
    "        \n",
    "        def SetParams(self):\n",
    "            self.fn = Bump()\n",
    "            self.fn.SetParams()\n",
    "            \n",
    "        def EvalValue(self, x):\n",
    "            return self.fn.EvalValue(x)\n",
    "        \n",
    "    # In RLlib, you need the config arg\n",
    "    def __init__(self,config):\n",
    "        self.meshfile = 'inline-quad.mesh'\n",
    "        \n",
    "        # keep a copy of the unrefined mesh so we can restore it\n",
    "        self.mesh0 = mfem.Mesh(self.meshfile)\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        \n",
    "        # The only reason we need to create a fespace and gf here\n",
    "        # is to find the sizes needed for the action and observation spaces\n",
    "        dim = self.mesh.Dimension()\n",
    "        self.order = 1\n",
    "        self.fec = mfem.L2_FECollection(self.order, dim)\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        self.u = mfem.GridFunction(self.fes);\n",
    "\n",
    "        # actions are: refine each element, or do nothing\n",
    "        self.action_space = spaces.Discrete(self.mesh.GetNE())\n",
    "        self.observation_space = spaces.Box(-1.0, 1.0, shape=(self.u.Size(),), dtype=np.float32)\n",
    "        self.state = None\n",
    "        \n",
    "        # call reset to create the first synthetic function\n",
    "        self.reset()\n",
    "        \n",
    "        #self.gl = GlvisWidget(get_solnstream(self.mesh,self.u))\n",
    "        \n",
    "    def get_ne(self):\n",
    "        return self.mesh.GetNE()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.u.Size()\n",
    "    \n",
    "    # Compute L2 error wrt to the analytic fn definition\n",
    "    def get_error(self):\n",
    "        err = self.u.ComputeL2Error(self.u0)\n",
    "        return err\n",
    "    \n",
    "    # Manually refine the elements in the array elems\n",
    "    def refine_elems(self, elems):\n",
    "        self.mesh.GeneralRefinement(mfem.intArray(elems))\n",
    "        self.fes.Update()\n",
    "        self.u.Update()\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "            \n",
    "    # action is the number of the element to refine\n",
    "    def step(self, action):\n",
    "        err1 = self.get_error()\n",
    "        self.refine_elems([action])\n",
    "        err2 = self.get_error()\n",
    "        reward = err1-err2\n",
    "        done = True\n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    # similar to reset, but do not choose a new function\n",
    "    def reinit(self):\n",
    "        del self.mesh\n",
    "        self.mesh = mfem.Mesh(self.mesh0)\n",
    "\n",
    "        del self.fes\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "\n",
    "        del self.u\n",
    "        self.u = mfem.GridFunction(self.fes)\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "        \n",
    "        self.state = self.u.GetDataArray()\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    # every reset of the env chooses a new synthetic function\n",
    "    def reset(self):\n",
    "        self.u0 = self.u0_coeff()\n",
    "        self.u0.SetParams()\n",
    "        return self.reinit()\n",
    "    \n",
    "    def render(self):\n",
    "        return GlvisWidget(get_solnstream(self.mesh,self.u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the environment and sanity check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMRGame(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_ne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008150442843904322"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, reward, done, info = env.step(0)\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show with refinement of element 0. Then we'll test resetting it to the original state.  We're going to need this to go through a searching for the best actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759aed99f56741c39f4e2dd56afb0810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887f2bd2ae74b79ada38d1d988d9972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reinit() # puts the mesh back in the orig state, and sets the DOF vector to u0\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, try training a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 20:48:36,589\tINFO trainer.py:588 -- Executing eagerly, with eager_tracing=False\n",
      "2021-02-08 20:48:36,593\tINFO trainer.py:618 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-02-08 20:48:39,098\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"] = \"1\"\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m 2021-02-08 20:48:39,108\tWARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m   arr = np.array(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8920)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8921)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward mean: 0.000190 \n",
      "CPU times: user 53.4 s, sys: 2.56 s, total: 55.9 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(1):\n",
    "    result = agent.train()\n",
    "    print(\"episode reward mean: %f \" % result[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          25856       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          25856       observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 25)           6425        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 189,978\n",
      "Trainable params: 189,978\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convenience function for applying a policy to a given observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_policy(model, obs):\n",
    "    action = agent.compute_action(obs)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #print(\"policy chooses action %d with reward %f\" % (action, reward))\n",
    "    return action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force search for the best choice by trying each one, remembering to reset the environment after each action and after we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_best_el(obs):\n",
    "    u0 = mfem.Vector(obs)\n",
    "    maxr = 0.0;\n",
    "    maxel = -1;\n",
    "    env.reinit()\n",
    "    ne = env.get_ne()\n",
    "    for n in range(ne):\n",
    "        env.reinit()\n",
    "        state, reward, done, info = env.step(n)\n",
    "        if reward > maxr:\n",
    "            maxr = reward\n",
    "            maxel = n\n",
    "    #print(\"max reward is %f by refining element %d\" % (maxr, maxel))\n",
    "    env.reinit()\n",
    "    return maxel, maxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27091e4c076a454caf5f36f8fd808380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "maxel, maxr = find_best_el(obs)\n",
    "env.refine_elems([maxel])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with what the policy does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3ae77cb1b54c07a62ff3dd9dd6a346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlvisWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reinit()\n",
    "apply_policy(model,obs)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a more systematic evaluation using an ensemble of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms error:  0.002662726747302056\n",
      "% correct:  5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.002662726747302056, 5.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_ensemble(model, ntrials):\n",
    "    ncorrect = 0\n",
    "    sumsq = 0\n",
    "    for n in range(ntrials):\n",
    "        obs = env.reset()\n",
    "        bestaction, bestreward = find_best_el(obs)\n",
    "        action, reward = apply_policy(model,obs)\n",
    "        err = bestreward-reward\n",
    "        sumsq += err*err\n",
    "        if (bestaction == action):\n",
    "            ncorrect += 1\n",
    "    rms = math.sqrt(sumsq/ntrials)\n",
    "    corr = 100.*ncorrect/ntrials\n",
    "    print(\"rms error: \",rms,flush=True)\n",
    "    print(\"% correct: \",corr,flush=True)\n",
    "    return rms, corr\n",
    "\n",
    "eval_ensemble(model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a few eval sample sizes to get a sense of how many are needed to estimate the metrics of the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms error:  0.003977529890565904\n",
      "% correct:  6.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.003977529890565904, 6.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ensemble(model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms error:  0.002905774742894879\n",
      "% correct:  5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.002905774742894879, 5.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ensemble(model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms error:  0.003378988760083888\n",
      "% correct:  5.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.003378988760083888, 5.25)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ensemble(model, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the training process is making progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8918)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8918)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8918)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-02-08 20:54:42,934\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=8918)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=8918)\u001b[0m   arr = np.array(v)\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m 2021-02-08 20:54:42,948\tWARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\u001b[2m\u001b[36m(pid=8919)\u001b[0m   arr = np.array(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 0 of size 50000\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 2e6\n",
    "nbatches = 40\n",
    "batch_size = total_episodes/nbatches\n",
    "neval = 200\n",
    "\n",
    "del agent\n",
    "config['train_batch_size'] = int(batch_size)\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)\n",
    "\n",
    "rms = [0.0] * nbatches\n",
    "cor = [0.0] * nbatches\n",
    "for n in range(nbatches):\n",
    "    print(\"training batch %d of size %d\" % (n,batch_size))\n",
    "    agent.train()\n",
    "    print(\"evaluating on %d instances...\" %  neval)\n",
    "    rms[n], cor[n] = eval_ensemble(model, neval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "isteps = list(range(nsteps))\n",
    "asteps = [i*config['train_batch_size'] for i in isteps]\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.subplot(211)\n",
    "ax.set_ylim(0.0001,0.1)\n",
    "ax.set_ylabel('RMS error')\n",
    "plt.semilogy(asteps,rms[:nsteps], marker='o')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax = plt.subplot(212)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('% correct')\n",
    "ax.set_xlabel('training episodes')\n",
    "plt.plot(asteps,cor[:nsteps], marker='o')\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigame",
   "language": "python",
   "name": "minigame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
