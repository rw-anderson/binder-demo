{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minigame 10a: Choose One Element To Refine\n",
    "\n",
    "This is essentially our \"global environment\" for refinement where the action is choosing one element at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos,sin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "from gym import spaces, utils\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from os.path import expanduser, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glvis import glvis, to_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mfem import path\n",
    "import mfem.ser as mfem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some synthetic test functions: steps and bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x,theta):\n",
    "    x0 = x[0]\n",
    "    y0 = x[1]\n",
    "    x1 = x0*cos(theta)-y0*sin(theta)\n",
    "    y1 = x0*sin(theta)+y0*cos(theta)\n",
    "    return [x1,y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    x0 = x[0]\n",
    "    if (x0 < 0.0):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotated_step(x, theta):\n",
    "    xr = rotate(x,theta)\n",
    "    return step(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump(x):\n",
    "    rsq = x[0]**2 +x[1]**2\n",
    "    return math.exp(-rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_step(x):\n",
    "    return 0.5*(1.0 +math.tanh(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotated_smooth_step(x,theta):\n",
    "    xr = rotate(x,theta)\n",
    "    return smooth_step(xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classes where we can set the parameters and then eval a bunch of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.theta = random.uniform(0.0, 2.0*math.pi)\n",
    "        self.dx = [random.uniform(-1.0, 1.0),random.uniform(-1.0, 1.0)]\n",
    "        \n",
    "    def EvalValue(self, x):\n",
    "        return rotated_step(x+self.dx, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bump(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width = random.uniform(0.1,1.0)\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.dx = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "        y1 = random.uniform(0.1,0.9)\n",
    "        y2 = random.uniform(0.1,0.9)\n",
    "        self.floor = min(y1,y2)\n",
    "        self.ceiling = max(y1,y2)\n",
    "        self.height = self.ceiling -self.floor\n",
    "        \n",
    "    def Print(self):\n",
    "        print(\"width = %f\" % self.width)\n",
    "        print(\"xc = %f,%f\" % (self.xc[0],self.xc[1]))\n",
    "        print(\"dx = %f,%f\" % (self.dx[0],self.dx[1]))\n",
    "        print(\"floor = %f\" % self.floor)\n",
    "        print(\"ceil = %f\"  % self.ceiling)\n",
    "        print(\"height = %f\"% self.height)\n",
    "        \n",
    "    def EvalValue(self, x):\n",
    "        v = self.floor +self.height*bump((x-self.xc+self.dx)/self.width)\n",
    "        #assert v > 0.0\n",
    "        #assert v < 1.0\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBump(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width1 = random.uniform(0.1,0.5)\n",
    "        self.width2 = random.uniform(0.1,0.5)\n",
    "        self.xc1 = [0.5,0.5]\n",
    "        self.xc2 = [0.5,0.5]\n",
    "        self.dx1 = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "        self.dx2 = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return 0.5*(bump((x-self.xc1+self.dx1)/self.width1)+bump((x-self.xc2+self.dx2)/self.width2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothStep(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.width = random.uniform(5.0, 10.0)\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.dx = random.uniform(-0.5,0.5)\n",
    "        self.theta = random.uniform(0.0, 2.0*math.pi)\n",
    "        self.height = random.uniform(0.0, 1.0)\n",
    "        y1 = random.uniform(0.0,1.0)\n",
    "        y2 = random.uniform(0.0,1.0)\n",
    "        self.floor = min(y1,y2)\n",
    "        self.ceiling = max(y1,y2)\n",
    "        self.height = self.ceiling -self.floor\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        x -= self.xc\n",
    "        x += self.dx\n",
    "        return self.floor +self.height*rotated_smooth_step(x*self.width, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BumpAndSmoothStep(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        self.bump = Bump()\n",
    "        self.bump.SetParams()\n",
    "        self.smooth_step = SmoothStep()\n",
    "        self.smooth_step.SetParams()\n",
    "        self.alpha = random.uniform(0.0, 1.0)\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return self.alpha*self.bump.EvalValue(x)+ (1-self.alpha)*self.smooth_step.EvalValue(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BumpNarrowWide(mfem.PyCoefficient):\n",
    "    \n",
    "    def SetParams(self):\n",
    "        a = random.uniform(0.0,1.0)\n",
    "        if (a < 0.5):\n",
    "            self.width = 0.2\n",
    "            self.height = 1.0\n",
    "        else:\n",
    "            self.width = 0.4\n",
    "            self.height = 0.1\n",
    "        self.xc = [0.5,0.5]\n",
    "        self.dx = [random.uniform(-0.5, 0.5),random.uniform(-0.5, 0.5)]\n",
    "\n",
    "    def EvalValue(self, x):\n",
    "        return self.height*bump((x-self.xc+self.dx)/self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an instance of the test function. Note that each instance has randomly chosen parameters.  For the steps, it's a rotation angle and a displacement.  For the bumps, it's a width and a displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh1 = mfem.Mesh('inline-quad.mesh')\n",
    "mesh1.UniformRefinement()\n",
    "fec1 = mfem.H1_FECollection(p=1, dim=2)\n",
    "fes1 = mfem.FiniteElementSpace(mesh1, fec1)\n",
    "u1 = mfem.GridFunction(fes1)\n",
    "c1 = BumpAndSmoothStep()\n",
    "c1.SetParams()\n",
    "u1.ProjectCoefficient(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172e173505d4032aa2b4fb6d5e32897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glvis()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glvis(to_stream(mesh1,u1) + 'keys Rjlmc',600,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGame(gym.Env):\n",
    "    \n",
    "    class u0_coeff(mfem.PyCoefficient):\n",
    "        \n",
    "        def SetParams(self):\n",
    "            #self.fn = BumpAndSmoothStep()\n",
    "            self.fn = Bump()\n",
    "\n",
    "            self.fn.SetParams()\n",
    "            \n",
    "        def Print(self):\n",
    "            self.fn.Print()\n",
    "            \n",
    "        def EvalValue(self, x):\n",
    "            v = self.fn.EvalValue(x)\n",
    "            #assert v >= 0.0\n",
    "            #assert v <= 1.0\n",
    "            return self.fn.EvalValue(x)\n",
    "        \n",
    "    # In RLlib, you need the config arg\n",
    "    def __init__(self,config):\n",
    "        self.meshfile = 'inline-quad.mesh'\n",
    "        \n",
    "        # keep a copy of the unrefined mesh so we can restore it from memory\n",
    "        self.mesh0 = mfem.Mesh(self.meshfile)\n",
    "        self.mesh = mfem.Mesh(self.meshfile)\n",
    "        \n",
    "        # The only reason we need to create an fespace and gf here\n",
    "        # is to find the sizes needed for the action and observation spaces\n",
    "        dim = self.mesh.Dimension()\n",
    "        self.order = 1\n",
    "        self.fec = mfem.H1_FECollection(self.order, dim)\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "        self.u = mfem.GridFunction(self.fes);\n",
    "\n",
    "        # actions are: refine each element, or do nothing\n",
    "        self.action_space = spaces.Discrete(self.mesh.GetNE())\n",
    "        \n",
    "        # observation is a set of sample points forming a 2D image\n",
    "        n = int(math.sqrt(self.mesh.GetNE()))\n",
    "        self.obsx = 2*n\n",
    "        self.obsy = 2*n\n",
    "        self.obsx = 42\n",
    "        self.obsy = 42\n",
    "        \n",
    "        # add a little extra range on the space to account for interpolation errors\n",
    "        self.observation_space = spaces.Box(0, 255, shape=(self.obsx,self.obsy,1))\n",
    "        self.get_obs_points()\n",
    "        \n",
    "        self.n = 0\n",
    "        \n",
    "        # call reset to create the first synthetic function\n",
    "        self.reset()\n",
    "        \n",
    "        #self.gl = GlvisWidget(get_solnstream(self.mesh,self.u))\n",
    "        \n",
    "    # precompute the observation points and the elements and integration points we need\n",
    "    def get_obs_points(self):\n",
    "        n = math.sqrt(self.mesh.GetNE())\n",
    "        dx = 1.0/self.obsx\n",
    "        dy = 1.0/self.obsy\n",
    "        self.sample_pts = []\n",
    "        self.sample_els = []\n",
    "        self.sample_ips = []\n",
    "        for j in range(self.obsy):\n",
    "            for i in range(self.obsx):\n",
    "                pt = [i*dx+0.5*dx,j*dy+0.5*dy]\n",
    "                self.sample_pts.append(pt)\n",
    "                n, el, ip = self.mesh.FindPoints([pt])\n",
    "                #assert n == 1\n",
    "                #assert ip[0].x > 0.0\n",
    "                #assert ip[0].x < 1.0\n",
    "                #assert ip[0].y > 0.0\n",
    "                #assert ip[0].y < 1.0\n",
    "                #assert el[0] >= 0\n",
    "                #assert el[0] < self.mesh.GetNE()\n",
    "                # copy these so they won't be destroyed when mesh goes away?\n",
    "                ip0 = mfem.IntegrationPoint()\n",
    "                ip0.x = ip[0].x\n",
    "                ip0.y = ip[0].y\n",
    "                self.sample_els.append(el[0])\n",
    "                self.sample_ips.append(ip0)\n",
    "                \n",
    "    def get_obs(self):\n",
    "        state = np.empty((self.obsx,self.obsy,1))\n",
    "        k = 0\n",
    "        for j in range(self.obsy):\n",
    "            for i in range(self.obsx):\n",
    "                #assert k < len(self.sample_els)\n",
    "                #assert k < len(self.sample_ips)\n",
    "                el = self.sample_els[k]\n",
    "                #assert el >= 0\n",
    "                #assert el < self.mesh.GetNE()\n",
    "                ip = self.sample_ips[k]\n",
    "                #assert ip.x >= 0.0, \"k={}, i={}, j={}, x is {}\".format(k,i,j,ip.x)\n",
    "                #assert ip.x <= 1.0, \"k={}, i={}, j={}, x is {}\".format(k,i,j,ip.x)\n",
    "                #assert ip.y >= 0.0, \"k={}, i={}, j={}, y is {}\".format(k,i,j,ip.y)\n",
    "                #assert ip.y <= 1.0, \"k={}, i={}, j={}, y is {}\".format(k,i,j,ip.y)\n",
    "                v = self.u.GetValue(self.sample_els[k],self.sample_ips[k])\n",
    "                state[i][j] = 255.0*v\n",
    "                if (v > 1.0 or v < 0.0):\n",
    "                    print(\"element %d\" % self.sample_els[k])\n",
    "                    print(\"ip.x = %f\" % self.sample_ips[k].x)\n",
    "                    print(\"ip.y = %f\" % self.sample_ips[k].y)\n",
    "                    print(\"%d,%d -> %f\" % (i,j,v))\n",
    "                    print(\"%d,%d -> %f\" % (i,j,state[i][j]))\n",
    "                    self.u0.Print()\n",
    "                k += 1\n",
    "        self.state = state\n",
    "        return state\n",
    "                    \n",
    "    # Compute L2 error wrt to the analytic fn definition\n",
    "    def get_error(self):\n",
    "        err = self.u.ComputeL2Error(self.u0)\n",
    "        return err\n",
    "    \n",
    "    # Manually refine the elements in the array elems\n",
    "    def refine_elems(self, elems):\n",
    "        self.mesh.GeneralRefinement(mfem.intArray(elems))\n",
    "        self.fes.Update()\n",
    "        self.u.Update()\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "            \n",
    "    # action is the number of the element to refine\n",
    "    def step(self, action):\n",
    "        self.n += 1\n",
    "        #print(\"begin step %d: action is %d\" % (self.n,action))\n",
    "        err1 = self.get_error()\n",
    "        #print(\"err1 = %f\" % err1)\n",
    "        self.refine_elems([action])\n",
    "        err2 = self.get_error()\n",
    "        #print(\"err2 = %f\" % err2)\n",
    "        reward = err1-err2\n",
    "        done = True\n",
    "        #print(\"step reward = %f\" % reward)\n",
    "        #obs = self.get_obs()\n",
    "        \n",
    "        # use old state\n",
    "        return np.array(self.state), reward*1.e6, done, {}\n",
    "    \n",
    "    # similar to reset, but do not choose a new function\n",
    "    def reinit(self):\n",
    "        #print(\"reinit\")\n",
    "        del self.mesh\n",
    "        self.mesh = mfem.Mesh(self.mesh0)\n",
    "\n",
    "        del self.fes\n",
    "        self.fes = mfem.FiniteElementSpace(self.mesh, self.fec)\n",
    "\n",
    "        del self.u\n",
    "        self.u = mfem.GridFunction(self.fes)\n",
    "        self.u.ProjectCoefficient(self.u0)\n",
    "        \n",
    "        #self.get_obs_points()\n",
    "        obs = self.get_obs()\n",
    "        \n",
    "        return np.array(obs)\n",
    "    \n",
    "    # every reset of the env chooses a new synthetic function\n",
    "    def reset(self):\n",
    "        #print(\"reset\")\n",
    "        self.u0 = self.u0_coeff()\n",
    "        self.u0.SetParams()\n",
    "        return self.reinit()\n",
    "    \n",
    "    def render(self):\n",
    "        return glvis(to_stream(self.mesh,self.u) + 'keys Rjlmc',600,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the environment and sanity check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ad218069d74a8abcda521a175f2e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glvis()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = AMRGame(None)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7906c0fa30f748cc90dbfcc35953af56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glvis()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reinit()\n",
    "obs, reward, done, _ = env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, try training a policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['train_batch_size'] = int(1e4)\n",
    "config['num_workers'] = 3\n",
    "#config['kl_coeff']\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config['train_batch_size'] = int(2e2)\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "for n in range(1):\n",
    "    result = agent.train()\n",
    "    print(\"episode reward mean: %f \" % result[\"episode_reward_mean\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convenience function for applying a policy to a given observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_policy(model, obs):\n",
    "    action = agent.compute_action(obs, explore=False) # use deterministic mode\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #print(\"policy chooses action %d with reward %f\" % (action, reward))\n",
    "    return action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs = env.reset()\n",
    "action, reward = apply_policy(model, obs)\n",
    "action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs = env.reinit()\n",
    "action, reward = apply_policy(model, obs)\n",
    "action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force search for the best choice by trying each one, remembering to reset the environment after each action and after we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_optimal(obs):\n",
    "    u0 = mfem.Vector(obs)\n",
    "    maxr = 0.0;\n",
    "    maxel = -1;\n",
    "    env.reinit()\n",
    "    ne = env.mesh.GetNE()\n",
    "    for n in range(ne):\n",
    "        env.reinit()\n",
    "        state, reward, done, info = env.step(n)\n",
    "        if reward > maxr:\n",
    "            maxr = reward\n",
    "            maxel = n\n",
    "    #print(\"max reward is %f by refining element %d\" % (maxr, maxel))\n",
    "    env.reinit()\n",
    "    return maxel, maxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad6577ffde1439383f3f07c61436424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glvis()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "maxel, maxr = find_optimal(obs)\n",
    "env.refine_elems([maxel])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with what the policy does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "env.reinit()\n",
    "apply_policy(model,obs)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an error estimator based on the difference between the discontinuous and continuous representations. This is only valid for L2 FE spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dgjumps(env):\n",
    "    \n",
    "    mesh = env.mesh\n",
    "    u = env.u\n",
    "    \n",
    "    # put the L2 gridfunction into a coefficient so we can project it\n",
    "    u_disc_coeff = mfem.GridFunctionCoefficient(u)\n",
    "    h1_fec = mfem.H1_FECollection(p=1, dim=2)\n",
    "    h1_fes = mfem.FiniteElementSpace(mesh, h1_fec)\n",
    "    u_h1 = mfem.GridFunction(h1_fes)\n",
    "    u_h1.ProjectDiscCoefficient(u_disc_coeff, mfem.GridFunction.ARITHMETIC)\n",
    "    \n",
    "    # put the H1 smoothed function into a coefficient\n",
    "    u_h1_coeff = mfem.GridFunctionCoefficient(u_h1)\n",
    "    \n",
    "    # create a 0-order L2 field to hold errors\n",
    "    l2_0_fec = mfem.L2_FECollection(p=0,dim=2)\n",
    "    l2_0_fes = mfem.FiniteElementSpace(mesh,l2_0_fec)\n",
    "\n",
    "    # Compute elementwise \"errors\" between continuous and discontinuous fields\n",
    "    err_gf = mfem.GridFunction(l2_0_fes);\n",
    "    u.ComputeElementL2Errors(u_h1_coeff, err_gf);\n",
    "    \n",
    "    best_action = np.argmax(err_gf.GetDataArray())\n",
    "    \n",
    "    state, reward, done, info = env.step(best_action)\n",
    "    env.reinit()\n",
    "\n",
    "    return best_action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed43f341e0f7416e9d0d4c3dc06b03cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "glvis()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "action, reward = find_dgjumps(env)\n",
    "env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a more systematic evaluation using an ensemble of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(model, ntrials):\n",
    "    ncorrect = 0.0\n",
    "    sumsq = 0.0\n",
    "    maxerrsq = 0.0\n",
    "    dg_ncorrect = 0.0\n",
    "    dg_sumsq = 0.0\n",
    "    dg_maxerrsq = 0.0\n",
    "    for n in range(ntrials):\n",
    "        obs = env.reset()\n",
    "        bestaction, bestreward = find_optimal(obs)\n",
    "        dgaction, dgreward = find_dgjumps(env)\n",
    "        action, reward = apply_policy(model,obs)\n",
    "        err = bestreward-reward\n",
    "        maxerrsq = max(err*err,maxerrsq)\n",
    "        sumsq += err*err\n",
    "        dg_err = bestreward-dgreward\n",
    "        dg_maxerrsq = max(dg_err*dg_err,dg_maxerrsq)\n",
    "        dg_sumsq += dg_err*dg_err\n",
    "        if (bestaction == action):\n",
    "            ncorrect += 1\n",
    "        if (bestaction == dgaction):\n",
    "            dg_ncorrect += 1\n",
    "    rms = math.sqrt(sumsq/ntrials)\n",
    "    corr = 100.*ncorrect/ntrials\n",
    "    print(\"policy rms error: \",rms,flush=True)\n",
    "    print(\"policy max sq error: \",maxerrsq,flush=True)\n",
    "    print(\"policy % correct: \",corr,flush=True)\n",
    "    dg_rms = math.sqrt(dg_sumsq/ntrials)\n",
    "    dg_corr = 100.*dg_ncorrect/ntrials\n",
    "    print(\"dg rms error: \",dg_rms,flush=True)\n",
    "    print(\"dg max sq error: \",dg_maxerrsq,flush=True)\n",
    "    print(\"dg % correct: \",dg_corr,flush=True)\n",
    "    return rms, math.sqrt(maxerrsq), corr, dg_rms, math.sqrt(dg_maxerrsq), dg_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "eval_ensemble(model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a few eval sample sizes to get a sense of how many are needed to estimate the metrics of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_ensemble(model, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_ensemble(model, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the training process is making progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 10:16:08,217\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-02-25 10:16:10,577\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-02-25 10:16:10,578\tINFO trainer.py:618 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m non-resource variables are not supported in the long term\n",
      "2021-02-25 10:16:16,540\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m 2021-02-25 10:16:16,554\tWARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 0 of size 1000\n",
      "WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22303)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22304)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m WARNING:tensorflow:From /home/rwa/pyvenv/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:852: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=22302)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 1 of size 1000\n",
      "training batch 2 of size 1000\n",
      "training batch 3 of size 1000\n",
      "training batch 4 of size 1000\n",
      "training batch 5 of size 1000\n",
      "training batch 6 of size 1000\n",
      "training batch 7 of size 1000\n",
      "training batch 8 of size 1000\n",
      "training batch 9 of size 1000\n",
      "/home/rwa/ray_results/PPO_AMRGame_2021-02-25_10-16-108zsaagzh/checkpoint_10/checkpoint-10\n",
      "training batch 10 of size 1000\n",
      "training batch 11 of size 1000\n",
      "training batch 12 of size 1000\n",
      "training batch 13 of size 1000\n",
      "training batch 14 of size 1000\n",
      "training batch 15 of size 1000\n",
      "training batch 16 of size 1000\n",
      "training batch 17 of size 1000\n",
      "training batch 18 of size 1000\n",
      "training batch 19 of size 1000\n",
      "/home/rwa/ray_results/PPO_AMRGame_2021-02-25_10-16-108zsaagzh/checkpoint_20/checkpoint-20\n",
      "training batch 20 of size 1000\n",
      "training batch 21 of size 1000\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "total_episodes = 2.e6\n",
    "batch_size = 1000\n",
    "nbatches = int(total_episodes/batch_size)\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['train_batch_size'] = batch_size\n",
    "config['num_workers'] = 3\n",
    "#config['lr'] = 1.e-5\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=AMRGame)\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "rms = []\n",
    "cor = []\n",
    "maxerr = []\n",
    "\n",
    "dg_rms = []\n",
    "dg_cor = []\n",
    "dg_maxerr = []\n",
    "\n",
    "checkpoint_period = 10000\n",
    "\n",
    "neval = 400\n",
    "eval_period = 100000\n",
    "\n",
    "episode = 0\n",
    "checkpoint_episode = 0\n",
    "eval_episode = 0\n",
    "for n in range(nbatches):\n",
    "    print(\"training batch %d of size %d\" % (n,config['train_batch_size']))\n",
    "    agent.train()\n",
    "    episode += config['train_batch_size']\n",
    "    checkpoint_episode += config['train_batch_size']\n",
    "    if (checkpoint_episode >= checkpoint_period):\n",
    "        checkpoint_episode = 0\n",
    "        checkpoint_path = agent.save()\n",
    "        print(checkpoint_path)\n",
    "        \n",
    "    eval_episode += config['train_batch_size']\n",
    "    if (eval_episode >= eval_period):\n",
    "        eval_episode = 0\n",
    "        rms1, maxerr1, cor1, dg_rms1, dg_maxerr1, dg_cor1 = eval_ensemble(model, neval)\n",
    "        rms.append(rms1)\n",
    "        maxerr1.append(maxerr1)\n",
    "        cor.append(cor1)\n",
    "        dg_rms.append(dg_rms1)\n",
    "        dg_maxerr.append(dg_maxerr1)\n",
    "        dg_cor.append(dg_cor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "isteps = list(range(nbatches))\n",
    "asteps = [i*config['train_batch_size'] for i in isteps]\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.subplot(211)\n",
    "ax.set_ylim(0.00001,0.01)\n",
    "ax.set_ylabel('Error')\n",
    "line1, = plt.semilogy(asteps,rms[:nbatches], marker='o')\n",
    "line2, = plt.semilogy(asteps,dg_rms[:nbatches], marker='x')\n",
    "line3, = plt.semilogy(asteps,maxerr[:nbatches], marker='.')\n",
    "line4, = plt.semilogy(asteps,dg_maxerr[:nbatches], marker='+')\n",
    "\n",
    "line1.set_label('RL rms')\n",
    "line2.set_label('DG rms')\n",
    "line3.set_label('RL max')\n",
    "line4.set_label('DG max')\n",
    "ax.legend()\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_ylabel('% correct')\n",
    "ax.set_xlabel('training episodes')\n",
    "line1, = plt.plot(asteps,cor[:nbatches], marker='o')\n",
    "line2, = plt.plot(asteps,dg_cor[:nbatches], marker='x')\n",
    "line1.set_label('RL policy')\n",
    "line2.set_label('DG')\n",
    "ax.legend()\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for cases where the policy gets it right and the DG method gets it wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(500):\n",
    "    obs = env.reset()\n",
    "    opt_action, opt_reward = find_optimal(obs)\n",
    "    dg_action, dg_reward = find_dgjumps(env)\n",
    "    pol_action, pol_reward = apply_policy(model, obs)\n",
    "    if ((pol_action == opt_action) and (dg_action != opt_action)):\n",
    "        break\n",
    "env.reinit()\n",
    "env.step(pol_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reinit()\n",
    "env.step(dg_action)\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigame",
   "language": "python",
   "name": "minigame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
